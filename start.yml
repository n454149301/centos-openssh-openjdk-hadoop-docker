---
# 初始化配置
- hosts: 'hadoop'
  remote_user: 'root'
  become: false
  environment:
    JAVA_HOME: /root/openjdk
  vars:
    zookeeper_client_port: 2181
    zookeeper_clients:
      - node1:
        id: 1
        public_ip: 192.168.1.100
        ip: 192.168.1.100
        follower_port: 2888
        election_port: 3888
      - node2:
        id: 2
        public_ip: 192.168.1.101
        ip: 192.168.1.101
        follower_port: 2888
        election_port: 3888
      - node3:
        id: 3
        public_ip: 192.168.1.102
        ip: 192.168.1.102
        follower_port: 2888
        election_port: 3888
    hadoop_nodes:
      - hadoop-master
      - hadoop-slave1
      - hadoop-slave2
    hadoop_slaves:
      - hadoop-slave1
      - hadoop-slave2
    hadoop_etc_xml:
      /root/hadoop/etc/hadoop/core-site.xml: 
        - property:
          name: fs.defaultFS
          value: hdfs://hdfscluster
        - property:
          name: hadoop.tmp.dir
          value: /root/hadoop/tmp
        - property:
          name: fs.trash.interval
          value: 1440
        - property:
          name: hadoop.proxyuser.root.hosts
          value: '*'
        - property:
          name: hadoop.proxyuser.root.groups
          value: '*'
        - property:
          name: hadoop.http.staticuser.user
          value: admin
      /root/hadoop/etc/hadoop/hdfs-site.xml: 
        - property:
          name: dfs.nameservices
          value: hdfscluster
        - property:
          name: dfs.datanode.address
          value: 0.0.0.0:9866
        - property:
          name: dfs.client.failover.proxy.provider.hdfscluster
          value: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
        - property:
          name: dfs.ha.fencing.methods
          value: sshfence
        - property:
          name: dfs.ha.fencing.ssh.private-key-files
          value: /root/.ssh/id_rsa
        - property:
          name: dfs.journalnode.edits.dir
          value: /root/hadoop/dfs/journal
        - property:
          name: dfs.ha.automatic-failover.enabled
          value: true
        - property:
          name: dfs.replication
          value: 3
        - property:
          name: dfs.permissions.enabled
          value: false
        - property:
          name: dfs.namenode.name.dir
          value: /root/hadoop/dfs/name
        - property:
          name: dfs.datanode.data.dir
          value: /root/hadoop/dfs/data
        - property:
          name: dfs.webhdfs.enabled
          value: true
        - property:
          name: dfs.client.block.write.replace-datanode-on-failure.enable
          value: false
        - property:
          name: dfs.client.block.write.replace-datanode-on-failure.policy
          value: DEFAULT
        - property:
          name: dfs.support.append
          value: true
        - property:
          name: dfs.journalnode.http-address
          value: 0.0.0.0:8480
        - property:
          name: dfs.journalnode.rpc-address
          value: 0.0.0.0:8485
        - property:
          name: dfs.namenode.datanode.registration.ip-hostname-check
          value: false
      /root/hadoop/etc/hadoop/mapred-site.xml: 
        - property:
          name: mapreduce.framework.name
          value: yarn
      /root/hadoop/etc/hadoop/yarn-site.xml: 
        - property:
          name: yarn.nodemanager.aux-services
          value: mapreduce_shuffle
        - property:
          name: yarn.nodemanager.env-whitelist
          value: JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME
  tasks:
    - name: 'check all'
      ping:
    - name: 'yum install xmlstarlet, test task'
      yum: 
        name: xmlstarlet
        state: latest
    - name: 'set zookeeper ip arr'
      shell: echo {% for client in zookeeper_clients %}{{client.public_ip}}:{{zookeeper_client_port}} {%endfor%}
      register: zookeeper_clients_ip_arr
    # 初始化配置zookeeper
    - name: 'zookeeper client port'
      template:
        src: /root/template.j2/zookeeper/zoo.cfg.j2
        dest: /root/zookeeper/conf/zoo.cfg
    - name: 'zookeeper node id'
      shell: cat /proc/1/environ | tr '\0' '\n' | grep zookeeper_my_id | awk -F'=' '{print $2}' > /root/zookeeper/data/myid
    - name: 'hadoop core-site.xml 特殊配置'
      shell:
        xmlstarlet ed -L -d '/configuration/*' /root/hadoop/etc/hadoop/core-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/core-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'ha.zookeeper.quorum' /root/hadoop/etc/hadoop/core-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="ha.zookeeper.quorum"]' -t elem -n value -v '{{zookeeper_clients_ip_arr.stdout.split(" ") | join(",")}}' /root/hadoop/etc/hadoop/core-site.xml;
    - name: 'set hdfs-site namenodes.hdfscluster'
      shell: echo {% for node in hadoop_nodes %}nn{{loop.index}} {%endfor%}
      register: hdfs_namenodes_hdfscluster
    - name: 'set hdfs-site namenode.shared.edits.dir'
      shell: echo {% for node in hadoop_nodes %}{{node}}:8485 {%endfor%}
      register: hdfs_namenode_shared_edits_dir
    - name: 'hadoop hdfs-site.xml 特殊配置'
      shell:
        xmlstarlet ed -L -d '/configuration/*' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.ha.namenodes.hdfscluster' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.ha.namenodes.hdfscluster"]' -t elem -n value -v '{{hdfs_namenodes_hdfscluster.stdout.split(" ") | join(",")}}' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.namenode.shared.edits.dir' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.namenode.shared.edits.dir"]' -t elem -n value -v 'qjournal://{{hdfs_namenode_shared_edits_dir.stdout.split(" ") | join(";")}}/hdfscluster' /root/hadoop/etc/hadoop/hdfs-site.xml;
        {% for node in hadoop_nodes %}
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.namenode.rpc-address.hdfscluster.nn{{loop.index}}' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.namenode.rpc-address.hdfscluster.nn{{loop.index}}"]' -t elem -n value -v '{{node}}:9820' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.namenode.http-address.hdfscluster.nn{{loop.index}}' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.namenode.http-address.hdfscluster.nn{{loop.index}}"]' -t elem -n value -v '{{node}}:9870' /root/hadoop/etc/hadoop/hdfs-site.xml;
        {%endfor%}
    # 批量xml修改配置
    - name: 'xml修改配置文件'
      shell:
        echo "hehe";
        {% for xml_file_path, xml_file_vars in hadoop_etc_xml.items() %}
        {% for property_var in xml_file_vars %}
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' {{xml_file_path}};
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v '{{property_var.name}}' {{xml_file_path}};
        xmlstarlet ed -L -s '/configuration/property[name="{{property_var.name}}"]' -t elem -n value -v '{{property_var.value}}' {{xml_file_path}};
        {%endfor%}
        {%endfor%}
    - name: 'create workers'
      shell:
        echo '' > /root/hadoop/etc/hadoop/workers;
        {% for node in hadoop_nodes %}
        echo {{node}} >> /root/hadoop/etc/hadoop/workers;
        {%endfor%}
    - name: 'create slaves'
      shell:
        echo '' > /root/hadoop/etc/slaves;
        {% for node in hadoop_slaves %}
        echo {{node}} >> /root/hadoop/etc/slaves;
        {%endfor%}
    - name: 'insert hadoop environment'
      shell:
        echo 'export JAVA_HOME=/root/openjdk' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_HOME=/root/hadoop' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native"' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_SSH_OPTS="-p 10022"' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_DATANODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_NAMENODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_SECONDARYNAMENODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export YARN_RESOURCEMANAGER_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export YARN_NODEMANAGER_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export ZOOKEEPER_HOME=/root/zookeeper' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_ZKFC_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_JOURNALNODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
    - name: 'start zookeeper'
      shell: '/root/zookeeper/bin/zkServer.sh start'
- hosts: "master"
  remote_user: root
  become: false
  environment:
    JAVA_HOME: /root/openjdk
  tasks:
    - name: 'stop hadoop'
      shell: '/root/hadoop/sbin/stop-all.sh'
    - name: 'start dfs'
      shell: '/root/hadoop/sbin/start-dfs.sh'
    - name: 'start yarn'
      shell: '/root/hadoop/sbin/start-yarn.sh'
    - name: 'start historyserver'
      shell: '/root/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver'
    - name: 'start httpfs.sh'
      shell: '/root/hadoop/sbin/httpfs.sh start'
    # 结束
    - name: 'wait'
      shell: 'tail -f /dev/null'
