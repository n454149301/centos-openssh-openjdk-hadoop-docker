---
# 初始化配置
- hosts: 'hadoop'
  ignore_unreachable: no
  any_errors_fatal: true
  remote_user: 'root'
  become: false
  environment:
    JAVA_HOME: /root/openjdk
  tasks:
    - name: 'check all'
      ping:
      ignore_errors: no
    - debug: var=zookeeper_my_id
    # 初始化配置zookeeper
    - name: 'zookeeper client port'
      template:
        src: /root/template.j2/zookeeper/zoo.cfg.j2
        dest: /root/zookeeper/conf/zoo.cfg
    - name: 'set zookeeper ip arr'
      shell: echo {% for client in groups["hadoop"] %}{{client}}:{{zookeeper_client_port}} {%endfor%}
      register: zookeeper_clients_ip_arr
    - name: 'zookeeper node id'
      shell: echo {{zookeeper_my_id}} > /root/zookeeper/data/myid
    - name: 'hadoop core-site.xml 特殊配置'
      shell:
        xmlstarlet ed -L -d '/configuration/*' /root/hadoop/etc/hadoop/core-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/core-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'ha.zookeeper.quorum' /root/hadoop/etc/hadoop/core-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="ha.zookeeper.quorum"]' -t elem -n value -v '{{zookeeper_clients_ip_arr.stdout.split(" ") | join(",")}}' /root/hadoop/etc/hadoop/core-site.xml;
    - name: 'set hdfs-site namenodes.hdfscluster'
      shell: echo {% for node in groups["hadoop"] %}nn{{loop.index}} {%endfor%}
      register: hdfs_namenodes_hdfscluster
    - name: 'set hdfs-site namenode.shared.edits.dir'
      shell: echo {% for node in groups["hadoop"] %}{{node}}:8485 {%endfor%}
      register: hdfs_namenode_shared_edits_dir
    - name: 'hadoop hdfs-site.xml 特殊配置'
      shell:
        xmlstarlet ed -L -d '/configuration/*' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.ha.namenodes.hdfscluster' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.ha.namenodes.hdfscluster"]' -t elem -n value -v '{{hdfs_namenodes_hdfscluster.stdout.split(" ") | join(",")}}' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.namenode.shared.edits.dir' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.namenode.shared.edits.dir"]' -t elem -n value -v 'qjournal://{{hdfs_namenode_shared_edits_dir.stdout.split(" ") | join(";")}}/hdfscluster' /root/hadoop/etc/hadoop/hdfs-site.xml;
        {% for node in groups["hadoop"] %}
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.namenode.rpc-address.hdfscluster.nn{{loop.index}}' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.namenode.rpc-address.hdfscluster.nn{{loop.index}}"]' -t elem -n value -v '{{node}}:9820' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v 'dfs.namenode.http-address.hdfscluster.nn{{loop.index}}' /root/hadoop/etc/hadoop/hdfs-site.xml;
        xmlstarlet ed -L -s '/configuration/property[name="dfs.namenode.http-address.hdfscluster.nn{{loop.index}}"]' -t elem -n value -v '{{node}}:9870' /root/hadoop/etc/hadoop/hdfs-site.xml;
        {%endfor%}
    # 批量xml修改配置
    - name: 'xml修改配置文件'
      shell:
        echo "hehe";
        {% for xml_file_path, xml_file_vars in hadoop_etc_xml.items() %}
        {% for property_var in xml_file_vars %}
        xmlstarlet ed -L -s '/configuration' -t elem -n property -v '' {{xml_file_path}};
        xmlstarlet ed -L -s '/configuration/property[last()]' -t elem -n name -v '{{property_var.name}}' {{xml_file_path}};
        xmlstarlet ed -L -s '/configuration/property[name="{{property_var.name}}"]' -t elem -n value -v '{{property_var.value}}' {{xml_file_path}};
        {%endfor%}
        {%endfor%}
    - name: 'create workers'
      shell:
        echo '' > /root/hadoop/etc/hadoop/workers;
        {% for node in groups["hadoop"] %}
        echo {{node}} >> /root/hadoop/etc/hadoop/workers;
        {%endfor%}
    - name: 'create slaves'
      shell:
        echo '' > /root/hadoop/etc/slaves;
        {% for node in groups["hadoop"] %}
        echo {{node}} >> /root/hadoop/etc/slaves;
        {%endfor%}
    - name: 'check found hadoop-env.demo.sh'
      shell:
        cp /root/hadoop/etc/hadoop/hadoop-env.demo.sh /root/hadoop/etc/hadoop/hadoop-env.sh
      when: hadoop_env_demo_path is exists
    - name: 'check found httpfs-env.demo.sh'
      shell:
        cp /root/hadoop/etc/hadoop/httpfs-env.demo.sh /root/hadoop/etc/hadoop/httpfs-env.sh
      when: httpfs_env_demo_path is exists
    - name: 'insert hadoop environment'
      shell:
        echo 'export JAVA_HOME=/root/openjdk' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_HOME=/root/hadoop' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native"' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HADOOP_SSH_OPTS="-p 10022"' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_DATANODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_NAMENODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_SECONDARYNAMENODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export YARN_RESOURCEMANAGER_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export YARN_NODEMANAGER_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export ZOOKEEPER_HOME=/root/zookeeper' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_ZKFC_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo 'export HDFS_JOURNALNODE_USER=root' >> /root/hadoop/etc/hadoop/hadoop-env.sh;
        echo "" >> /root/hadoop/etc/hadoop/httpfs-env.sh;
        echo 'export HTTPFS_HTTP_PORT=9900' >> /root/hadoop/etc/hadoop/httpfs-env.sh;
        echo 'export HTTPFS_TEMP=$HADOOP_HOME/tmp' >> /root/hadoop/etc/hadoop/httpfs-env.sh;
    - name: 'start zookeeper(因为可能出现子节点启动过的问题，强制正常退出)'
      shell: '/root/zookeeper/bin/zkServer.sh start; exit 0'
- hosts: "master"
  ignore_unreachable: no
  any_errors_fatal: true
  remote_user: root
  become: false
  environment:
    JAVA_HOME: /root/openjdk
  tasks:
    - name: 'stop hadoop'
      shell: '/root/hadoop/sbin/stop-all.sh'
    - name: 'start dfs'
      shell: '/root/hadoop/sbin/start-dfs.sh'
    - name: 'wait 20s'
      shell: 'sleep 20'
    - name: 'start yarn'
      shell: '/root/hadoop/sbin/start-yarn.sh'
    - name: 'wait 20s'
      shell: 'sleep 20'
    - name: 'start historyserver'
      shell: '/root/hadoop/bin/mapred --daemon start historyserver'
    - name: 'wait 20s'
      shell: 'sleep 20'
    - name: 'set haadmin nn1 to active'
      shell: 'yes | /root/hadoop/bin/hdfs haadmin -transitionToActive nn1 --forcemanual; exit 0'
    - name: 'wait 20s'
      shell: 'sleep 20'
    - name: 'start httpfs.sh'
      shell: '/root/hadoop/bin/hdfs --daemon start httpfs'
    # 结束
    - name: 'wait while check daemon.sh'
      shell: '/root/daemon.sh'
